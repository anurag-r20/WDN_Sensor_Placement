{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl98I0MoGD9S"
      },
      "source": [
        "<a name=\"top\" id=\"top\"></a>\n",
        "\n",
        "<div align=\"center\">\n",
        "    <h1>Sensor Placement and Fault Diagnosis in Real Water Distribution Netwroks</h1>\n",
        "    <a href=\"https://github.com/anurag-r20\">Anurag Ramesh</a>\n",
        "    <br>\n",
        "    <i>Graduate Research Assistant</i>\n",
        "    <br>\n",
        "    <i>Davidson School of Chemical Engineering, Purdue University</i>\n",
        "    <br>\n",
        "    <br>\n",
        "    <a href=\"https://github.com/bernalde\">David E. Bernal Neira</a>\n",
        "    <br>\n",
        "    <i> Assistant Professor </i>\n",
        "    <br>\n",
        "    <i>Davidson School of Chemical Engineering, Purdue University</i>\n",
        "    <br>\n",
        "    <br>\n",
        "   <a href=\"https://colab.research.google.com/github.com/anurag-r20/WDN_Sensor_Placement/WDN_Sensor_Placement.ipynb\">\n",
        "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "    </a>\n",
        "    <a href=\"https://secquoia.github.io/\">\n",
        "        <img src=\"https://img.shields.io/badge/ðŸŒ²âš›ï¸ðŸŒ-SECQUOIA-blue\" alt=\"SECQUOIA\"/>\n",
        "    </a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxqSZQD4GD9T"
      },
      "source": [
        "## Sensor Placement in Water Distribution Networks (WDN)\n",
        "\n",
        "In this notebook, we formulate an optimization problem to identify the optimal placement of pressure sensors in a Water Distribution Network (WDN). The WDN is represented on a constrained graph $G(V,E)$, where $V$ is the set of vertices and $E$ is the set of edges.\n",
        "\n",
        "We consider two different formulations of the optimization problem, including Mixed Integer Programming (MIP), Mixed Integer Quadratic Programming (MIQP), and Quadratic Unconstrained Binary Optimization (QUBO). The optimization problem is solved using three different solvers: Gurobi, Simulated Annealing, and D-Wave's implementation of Quantum Annealing via **[neal](https://github.com/dwavesystems/dwave-neal)**.\n",
        "\n",
        "Additionally, we leverage D-Wave's package **[dwavebinarycsp](https://github.com/dwavesystems/dwavebinarycsp)** to translate constraint satisfaction problems into QUBOs. For Groebner basis computations, we use **[Sympy](https://www.sympy.org/)** for symbolic computation in Python and **[Networkx](https://networkx.github.io/)** for network models and graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCP3i9ojGD9U"
      },
      "outputs": [],
      "source": [
        "# If using this on Google collab, we need to install the packages\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "# Let's install dimod, neal, and pyomo\n",
        "if IN_COLAB:\n",
        "    !pip install -q pyomo\n",
        "    !pip install dimod\n",
        "    !pip install dwave-neal\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install dwave-ocean-sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y73FqAR9GD9U"
      },
      "outputs": [],
      "source": [
        "# Import the Pyomo library, which can be installed via pip, conda or from Github https://github.com/Pyomo/pyomo\n",
        "import pyomo.environ as pyo\n",
        "# Import the Dwave packages dimod and neal\n",
        "import dimod\n",
        "import neal\n",
        "from tabu import TabuSampler\n",
        "# Import Matplotlib to generate plots\n",
        "import matplotlib.pyplot as plt\n",
        "# Import numpy and scipy for certain numerical calculations below\n",
        "import numpy as np\n",
        "from scipy.special import gamma\n",
        "import pandas as pd\n",
        "pd.options.display.float_format = '{:.6f}'.format\n",
        "import os\n",
        "import time\n",
        "import operator\n",
        "from collections import OrderedDict\n",
        "from collections import Counter\n",
        "# Import networkx for graph problems\n",
        "import networkx as nx\n",
        "from matplotlib.lines import Line2D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def WDN_network_data(city, base_dir=None):\n",
        "\n",
        "    \"\"\"Fetches and processes Water Distribution Network (WDN) data for a given city.\n",
        "\n",
        "    This function reads node, edge, and water consumption data from specified files\n",
        "    for a given city and returns the data as dictionaries.\n",
        "\n",
        "    Args:\n",
        "        city (str): The name of the city to fetch data for. Should be one of 'Apulia', \n",
        "            'Fossolo', or 'Test'.\n",
        "        base_dir (str, optional): The base directory where the data files are located. \n",
        "            Defaults to the directory of the script.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing three dictionaries:\n",
        "            - nodes (dict): Dictionary of nodes with node names as keys and (x, y) \n",
        "              coordinates as values.\n",
        "            - edges (dict): Dictionary of edges with tuples of node pairs as keys.\n",
        "            - water_consumption (dict): Dictionary of water consumption with node names\n",
        "              as keys and consumption values as values.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If any of the data files do not exist.\n",
        "        ValueError: If the provided city name is not valid.\n",
        "\n",
        "    Examples:\n",
        "        >>> nodes, edges, water_consumption = WDN_network_data('Apulian')\n",
        "        >>> print(nodes)\n",
        "        {'J1': (10.0, 20.0), 'J2': (15.0, 25.0)}\n",
        "    \"\"\"\n",
        "\n",
        "    # Set base directory\n",
        "    if base_dir is None:\n",
        "        base_dir = os.path.dirname(os.path.abspath(__file__))  # Set to script's directory\n",
        "    \n",
        "    data_dir = os.path.join(base_dir, 'Data/WDN_Data')\n",
        "    city_files = {\n",
        "        \"Apulia\": {\n",
        "            \"nodes\": f\"{data_dir}/Apulia_WDN/Apulia_WDN_Nodes.txt\",\n",
        "            \"edges\": f\"{data_dir}/Apulia_WDN/Apulia_WDN_Edges.txt\",\n",
        "            \"water_consumption\": f\"{data_dir}/Apulia_WDN/Apulia_WDN_Water_Consumption.txt\"\n",
        "        },\n",
        "        \"Fossolo\": {\n",
        "            \"nodes\": f\"{data_dir}/Fossolo_WDN/Fossolo_WDN_Nodes.txt\",\n",
        "            \"edges\": f\"{data_dir}/Fossolo_WDN/Fossolo_WDN_Edges.txt\",\n",
        "            \"water_consumption\": f\"{data_dir}/Fossolo_WDN/Fossolo_WDN_Water_Consumption.txt\"\n",
        "        },\n",
        "        \"Test\": {\n",
        "            \"nodes\": f\"{data_dir}/Test_WDN/Test_Nodes.txt\",\n",
        "            \"edges\": f\"{data_dir}/Test_WDN/Test_Edges.txt\",\n",
        "            \"water_consumption\": f\"{data_dir}/Test_WDN/Test_Water_Consumption.txt\"\n",
        "        },\n",
        "        \"ZJ\": {\n",
        "            \"nodes\": f\"{data_dir}/ZJ_WDN/ZJ_WDN_Nodes.txt\",\n",
        "            \"edges\": f\"{data_dir}/ZJ_WDN/ZJ_WDN_Edges.txt\",\n",
        "            \"water_consumption\": f\"{data_dir}/ZJ_WDN/ZJ_WDN_Water_Consumption.txt\"\n",
        "        },\n",
        "        \"Modena\": {\n",
        "            \"nodes\": f\"{data_dir}/Modena_WDN/Modena_WDN_Nodes.txt\",\n",
        "            \"edges\": f\"{data_dir}/Modena_WDN/Modena_WDN_Edges.txt\",\n",
        "            \"water_consumption\": f\"{data_dir}/Modena_WDN/Modena_WDN_Water_Consumption.txt\"\n",
        "        },\n",
        "        \"Kentucky\": {\n",
        "            \"nodes\": f\"{data_dir}/Kentucky_WDN/Kentucky_WDN_Nodes.txt\",\n",
        "            \"edges\": f\"{data_dir}/Kentucky_WDN/Kentucky_WDN_Edges.txt\",\n",
        "            \"water_consumption\": f\"{data_dir}/Kentucky_WDN/Kentucky_WDN_Water_Consumption.txt\"\n",
        "        }\n",
        "        #  \"Test\": {\n",
        "        #     \"nodes\": f\"{data_dir}/Test_WDN/Test_Nodes_5.txt\",\n",
        "        #     \"edges\": f\"{data_dir}/Test_WDN/Test_Edges_5.txt\",\n",
        "        #     \"water_consumption\": f\"{data_dir}/Test_WDN/Test_Water_Consumption_5.txt\"\n",
        "        # }      \n",
        "    }\n",
        "\n",
        "    # Check if the city is valid\n",
        "    if city not in city_files:\n",
        "        print(f\"Error: Unknown city {city}.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Get file paths\n",
        "    nodes_file_path = city_files[city][\"nodes\"]\n",
        "    edges_file_path = city_files[city][\"edges\"]\n",
        "    water_consumption_file_path = city_files[city][\"water_consumption\"]\n",
        "\n",
        "    # Print absolute file paths for debugging\n",
        "    print(f\"Nodes file path: {nodes_file_path}\")\n",
        "    print(f\"Edges file path: {edges_file_path}\")\n",
        "    print(f\"Water consumption file path: {water_consumption_file_path}\")\n",
        "\n",
        "    # Check if files exist\n",
        "    for file_path in [nodes_file_path, edges_file_path, water_consumption_file_path]:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file {file_path} does not exist.\")\n",
        "            return None, None, None\n",
        "\n",
        "    # Importing node data\n",
        "    nodes_df = pd.read_csv(nodes_file_path, skiprows=1, names=[\"Node\", \"X\", \"Y\"]) # nodes dataframe (exclude heading)\n",
        "    nodes_df['Node'] = nodes_df['Node'].str.strip() # strip spaces\n",
        "    nodes = {row['Node']: (float(row['X']), float(row['Y'])) for i, row in nodes_df.iterrows()} # Convert the data to a dictionary with node names as keys and (x, y) tuples as values.\n",
        "\n",
        "    # Importing edge data\n",
        "    edges_df = pd.read_csv(edges_file_path, skiprows=1, names=[\"Node1\", \"Node2\"]) # edges dataframe\n",
        "    edges_df['Node1'] = edges_df['Node1'].str.strip() # strip spaces\n",
        "    edges_df['Node2'] = edges_df['Node2'].str.strip()\n",
        "    edges = {((row['Node1']), (row['Node2'])) for i, row in edges_df.iterrows()} # Convert the data to a dictionary with node names as keys and (x, y) tuples as values.\n",
        "    \n",
        "    # Importing water consumption data\n",
        "    water_consumption_df = pd.read_csv(water_consumption_file_path, skiprows=1, names=[\"Node\", \"Consumption\"]) # water consumption dataframe\n",
        "    water_consumption_df['Node'] = water_consumption_df['Node'].str.strip() # strip spaces\n",
        "    water_consumption = {row['Node']: (float(row['Consumption'])) for i, row in water_consumption_df.iterrows()} # Convert the data to a dictionary with node names as keys and (x, y) tuples as values.\n",
        "\n",
        "    return nodes, edges, water_consumption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_dir = '/mnt/c/Users/rames102/Desktop/Qualifier_2024'  # relpace with correct directory in your system\n",
        "city = \"Apulia\"\n",
        "coords, edges, water_consumption = WDN_network_data(city, base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Construct_Graph(city):\n",
        "\n",
        "    \"\"\"Constructs a Water Distribution Network (WDN) graph for a given city.\n",
        "\n",
        "    This function constructs a graph using the node coordinates and edges imported previously.\n",
        "    It creates a NetworkX graph, adds nodes with their coordinates as attributes, and adds edges.\n",
        "\n",
        "    Args:\n",
        "        city (str): The name of the city for which to construct the graph. The function assumes\n",
        "            that the node coordinates and edges for the specified city have been imported.\n",
        "\n",
        "    Returns:\n",
        "        nx.Graph: A NetworkX graph object representing the WDN. If the coordinates or edges are\n",
        "            not available, returns None.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the city name is not valid or if the necessary data is missing.\n",
        "\n",
        "    Examples:\n",
        "        >>> G = Construct_Graph('Apulian')\n",
        "        >>> print(G.nodes(data=True))\n",
        "        [('J1', {'pos': (10.0, 20.0)}), ('J2', {'pos': (15.0, 25.0)})]\n",
        "    \"\"\"\n",
        "\n",
        "    if coords is None or edges is None:\n",
        "        return None\n",
        "    \n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Defining Nodes in their coordinates\n",
        "    for node, coord in coords.items():\n",
        "        G.add_node(node, pos=coord)\n",
        "\n",
        "    # Add edges\n",
        "    G.add_edges_from(edges)\n",
        "    \n",
        "    return G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "G = Construct_Graph(city)\n",
        "print(G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def centrality(G, water_consumption):\n",
        "\n",
        "    \"\"\"Calculates centrality metrics for a given city's WDN graph and water consumption data.\n",
        "\n",
        "    This function computes various centrality metrics for the provided NetworkX graph and calculates\n",
        "    vertex costs based on water consumption data and centrality measures.\n",
        "\n",
        "    Args:\n",
        "        G (nx.Graph): A NetworkX graph representing the water distribution network (WDN) for the city.\n",
        "        water_consumption (dict): A dictionary where keys are node names and values are the water consumption\n",
        "            at each node.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two elements:\n",
        "            - vertex_cost (dict): Dictionary where keys are node names and values are the computed costs\n",
        "              for each vertex based on normalized demand and degree centrality.\n",
        "            - edge_betweenness (dict): Dictionary of edge betweenness centrality values, where keys are\n",
        "              tuples representing edges and values are the betweenness centrality scores for those edges.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If a node from the graph is not found in the water consumption data.\n",
        "\n",
        "    Examples:\n",
        "        >>> vertex_cost, edge_betweenness = centrality(G, water_consumption)\n",
        "        >>> print(vertex_cost)\n",
        "        {'J1': 2.5, 'J2': 3.0}\n",
        "        >>> print(edge_betweenness)\n",
        "        {(J1, J2): 0.1, (J2, J3): 0.2}\n",
        "    \"\"\"\n",
        "    \n",
        "    # Calculating edge betweenness centrality\n",
        "    edge_betweenness = nx.edge_betweenness_centrality(G)\n",
        "\n",
        "    # Calculate degree centrality\n",
        "    degree_centrality = nx.degree_centrality(G)\n",
        "\n",
        "    # Edge betweenness centrality as weights\n",
        "    for (u, v, d) in G.edges(data=True):\n",
        "        d['weight'] = edge_betweenness[(u, v)]\n",
        "\n",
        "    # Define the weights C and D (Need to identify the right values)\n",
        "    C = 1.0\n",
        "    D = 1.0\n",
        "\n",
        "    def compute_normalized_demand(water_consumption):        \n",
        "        max_demand = max(water_consumption.items(), key=operator.itemgetter(1))[1]\n",
        "        return {node: val / max_demand for node, val in water_consumption.items()}\n",
        "\n",
        "    normalized_demand = compute_normalized_demand(water_consumption)\n",
        "\n",
        "    # Debugging: print the node names in the graph\n",
        "    print(\"Graph nodes:\", list(G.nodes()))\n",
        "    \n",
        "    # Debugging: print the keys in normalized_demand\n",
        "    print(\"Normalized demand keys:\", list(normalized_demand.keys()))\n",
        "\n",
        "    # Vertex cost function:\n",
        "    vertex_cost = {}\n",
        "    for node in G.nodes():\n",
        "        f_i = normalized_demand.get(node, None)   # function of the water need at each node i\n",
        "        if f_i is None:\n",
        "            print(f\"Node {node} not found in normalized_demand\")\n",
        "            continue\n",
        "        g_i = degree_centrality[node] / (len(G.nodes()) - 1)    # node weights\n",
        "        vertex_cost[node] = C * f_i + D * g_i\n",
        "    \n",
        "    return vertex_cost, edge_betweenness\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VC, EB = centrality(G, water_consumption)\n",
        "print(VC)\n",
        "print(EB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_WDN(G):\n",
        "\n",
        "    \"\"\"Plots the graph of the city's Water Distribution Network (WDN).\n",
        "\n",
        "    This function visualizes the WDN graph using NetworkX and Matplotlib. It plots nodes with their\n",
        "    positions, edges, and labels for both nodes and edges. It also includes a legend and title for the plot.\n",
        "\n",
        "    Args:\n",
        "        G (nx.Graph): A NetworkX graph object representing the water distribution network (WDN) for the city.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the graph `G` is None or the position dictionary is empty.\n",
        "\n",
        "    Examples:\n",
        "        >>> plot_WDN(G)\n",
        "    \"\"\"\n",
        "\n",
        "    if G is None:\n",
        "        print(f\"Graph could not be constructed for {city}.\")\n",
        "        return\n",
        "\n",
        "    pos = nx.get_node_attributes(G, 'pos')\n",
        "    \n",
        "    if not pos:\n",
        "        print(\"Error: Position dictionary is empty.\")\n",
        "        return\n",
        "\n",
        "    print(\"Position Dictionary:\", pos)\n",
        "\n",
        "    # Create plot\n",
        "    plt.figure(figsize=(24, 16))\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=500, node_color='skyblue')\n",
        "    nx.draw_networkx_edges(G, pos, edge_color='black')\n",
        "\n",
        "    # Round off edge and node labels to two decimals for the plot\n",
        "    edge_labels_rounded = {k: round(v, 2) for k, v in EB.items()}\n",
        "    node_labels_rounded = {k: round(v, 2) for k, v in VC.items()}\n",
        "\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels_rounded, label_pos=0.5, font_size=6, verticalalignment='bottom')\n",
        "    nx.draw_networkx_labels(G, pos, labels=node_labels_rounded, font_size=8, font_color='black')\n",
        "\n",
        "    # Add legend\n",
        "    blue_patch = Line2D([0], [0], marker='o', color='w', markerfacecolor='skyblue', markersize=10, label='Potential nodes for sensor placement')\n",
        "    plt.legend(handles=[blue_patch], loc='best', fontsize='24')\n",
        "\n",
        "    # plt.title(f\"Water Distribution Network - {city}\", fontsize = 80)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_WDN(G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The solution to this graph problem is explored using different reformulations of the problem.\n",
        "\n",
        "First, we formulate the graph optimization problem as a Mixed Integer Program (MIP) (Speziali et al. (2021)):\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{array}{rl}\n",
        "    \\displaystyle%\n",
        "    \\min_{x} & \\sum_{i \\in V} c_{i}x_{i} + \\sum_{(i, j) \\in E} w_{ij}(1 - x_{j} - x_{i} + x_{i}x_{j}) \\\\\n",
        "    \\textrm{s.t.} & \\sum_{i \\in V} x_{i} = s \\\\\n",
        "    & x_{i} \\in \\{0,1 \\}^{n}\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Here, $c_{i}$ represents the cost of the $i^{th}$ node, $w_{ij}$ represents the weight corresponding to the edge between nodes $i$ and $j$, and $x_{i} \\in \\{0, 1\\}^{n}$ is a binary decision variable that indicates whether a sensor is placed at the $i^{th}$ node. $s$ is the predefined total number of sensors.\n",
        "\n",
        "Next, the Quadratic Unconstrained Binary Optimzation (QUBO) formulation of the graph problem is explored. Consider the MIP model from before:\n",
        "$$\n",
        "\\begin{array}{rl}\n",
        "    \\displaystyle%\n",
        "    \\min_{x} & \\sum_{i \\in V} c_{i}x_{i} + \\sum_{(i, j) \\in E} w_{ij}(1 - x_{j} - x_{i} + x_{i}x_{j}) \\\\\n",
        "    \\textrm{s.t.} & \\sum_{i \\in V} x_{i} = s \\\\\n",
        "    & x_{i} \\in \\{0,1 \\}^{n}\n",
        "\\end{array}\n",
        "$$\n",
        "To implement the problem as a QUBO, the constraint should be lifted up into the objective function such that the problem becomes unconstrained. This is performed as follows:\n",
        "$$\n",
        "\\begin{array}{rl}\n",
        "    \\displaystyle%\n",
        "    \\min_{x} & \\sum_{i \\in V} c_{i}x_{i} + \\sum_{(i, j) \\in E} (w_{ij}(1 - x_{j} - x_{i} + x_{i}x_{j})) + \\rho(\\sum_{i \\in V} x_{i} - s)^2 \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "where $\\rho$ is a scalar penalty term. Further simplification leads to\n",
        "$$\n",
        "\\begin{array}{rl}\n",
        "    \\displaystyle%\n",
        "    \\min_{x} & \\sum_{i \\in V} c_{i}x_{i} + \\sum_{(i, j) \\in E} w_{ij} - \\sum_{(i, j) \\in E} w_{ij}x_{j} - \\sum_{(i, j) \\in E} w_{ij}x_{i} + \\sum_{(i, j) \\in E} w_{ij}x_{i}x_{j} + \\rho\\sum_{i \\in V} x_{i}^2 - 2\\rho\\sum_{i \\in V} x_{i}s + \\rho s^2 \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "Since, $x_{i}$ is a binary variable, we can write $x_{i}$ as $x_{i}^2$. Taking this into account, the problem now becomes: \n",
        "$$\n",
        "\\begin{array}{rl}\n",
        "    \\displaystyle%\n",
        "    \\min_{x} & \\sum_{i \\in V} c_{i}x_{i}^2 + \\sum_{(i, j) \\in E}w_{ij} - \\sum_{(i, j) \\in E} w_{ij}x_{j}^2 - \\sum_{(i, j) \\in E} w_{ij}x_{i}^2 + \\sum_{(i, j) \\in E} w_{ij}x_{i}x_{j} + \\rho\\sum_{i \\in V} x_{i}^2 - 2\\rho\\sum_{i \\in V} x_{i}^2s + \\rho s^2 \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "Performing some algebraic manipulations, we have our QUBO problem.\n",
        "$$\n",
        "\\begin{array}{rl}\n",
        "    \\displaystyle%\n",
        "    \\min_{x} \\left( \\sum_{i \\in V} (c_i + \\rho - 2\\rho s - w_{ij})x_i^2 + \\sum_{(i,j) \\in E} w_{ij}x_i x_j + \\rho s^2 + \\sum_{(i,j) \\in E}{w_{ij}} \\right) \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "where $s$ is the total number of sensors that is predefined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_pyomo_model(G): \n",
        "\n",
        "    \"\"\"Initializes a Pyomo model for optimizing sensor placement in a city's Water Distribution Network (WDN).\n",
        "\n",
        "    This function sets up a Pyomo model with nodes, edges, and associated parameters such as demand,\n",
        "    vertex cost, and edge betweenness. It also defines the binary decision variables for sensor placement.\n",
        "\n",
        "    Args:\n",
        "        G (nx.Graph): A NetworkX graph object representing the water distribution network (WDN) for the city.\n",
        "\n",
        "    Returns:\n",
        "        pyo.ConcreteModel: A Pyomo ConcreteModel instance with sets, parameters, and decision variables defined.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the graph `G` is None.\n",
        "\n",
        "    Examples:\n",
        "        >>> model = create_pyomo_model(G)\n",
        "        >>> print(model)\n",
        "    \"\"\"\n",
        "\n",
        "    if G is None:\n",
        "        print(f\"{city} does not exist.\")\n",
        "        return None\n",
        "\n",
        "    # Create a pyomo model\n",
        "    model = pyo.ConcreteModel()\n",
        "\n",
        "    # Define pyomo sets for graph G\n",
        "    nodes = list(G.nodes())\n",
        "    edges = list(G.edges())\n",
        "    model.nodes = pyo.Set(initialize=nodes)\n",
        "    model.edges = pyo.Set(initialize=edges, dimen=2)\n",
        "\n",
        "    # Parameters\n",
        "    demand = water_consumption\n",
        "    model.demand = pyo.Param(model.nodes, initialize=demand, mutable=True)\n",
        "    model.c = pyo.Param(model.nodes, initialize=VC, mutable=True)  # vertex cost\n",
        "    model.w = pyo.Param(model.edges, initialize=EB, mutable=True)  # edge_betweenness for edge weights\n",
        "\n",
        "    # Binary decision variable\n",
        "    model.x = pyo.Var(model.nodes, within=pyo.Binary)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = create_pyomo_model(G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mixed Integer Program model (MIP)\n",
        "    \n",
        "def MIQP(G, model_in, s):\n",
        "\n",
        "    \"\"\"Generates a Mixed Integer Programming (MIP) model for the given sensor placement problem.\n",
        "\n",
        "    This function defines the objective function and constraints for the MIP model, aiming to optimize\n",
        "    sensor placement in the city's Water Distribution Network (WDN). The objective function minimizes\n",
        "    the total cost based on vertex costs and edge betweenness, while the constraint ensures that exactly\n",
        "    `s` sensors are placed.\n",
        "\n",
        "    Args:\n",
        "        G (nx.Graph): A NetworkX graph object representing the water distribution network (WDN) for the city.\n",
        "        model (pyo.ConcreteModel): A Pyomo ConcreteModel instance with sets and parameters defined.\n",
        "        s (int): The number of sensors to be placed in the network.\n",
        "\n",
        "    Returns:\n",
        "        pyo.ConcreteModel: The updated Pyomo model with the objective function and constraints added.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the model is None.\n",
        "\n",
        "    Examples:\n",
        "        >>> model = MIQP(G, model, 5)\n",
        "        >>> print(model.obj)\n",
        "    \"\"\"\n",
        "\n",
        "    if model_in is None:\n",
        "        return None\n",
        "\n",
        "    model = model_in.clone()\n",
        "    \n",
        "    # Define objective function\n",
        "    def objective_rule_MIQP(model):\n",
        "        return sum(model.c[i] * model.x[i] for i in model.nodes) + \\\n",
        "                sum(model.w[(i, j)] * (1 - model.x[i] - model.x[j] + model.x[i] * model.x[j]) for (i, j) in model.edges)\n",
        "\n",
        "    model.obj = pyo.Objective(rule=objective_rule_MIQP, sense=pyo.minimize)\n",
        "\n",
        "    # Constraints\n",
        "    def sensor_constraint_rule_MIQP(model):\n",
        "        return sum(model.x[i] for i in model.nodes) <= s\n",
        "\n",
        "    model.sensor_constraint_MIQP = pyo.Constraint(rule=sensor_constraint_rule_MIQP)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quadratic Unconstrained Binary Optimization model (QUBO)\n",
        "\n",
        "def QUBO(G, model_in, s, rho):\n",
        "\n",
        "    \"\"\"Generates a Quadratic Unconstrained Binary Optimization (QUBO) model for the given problem.\n",
        "\n",
        "    This function defines the objective function for the QUBO model, which aims to optimize sensor placement\n",
        "    in the city's Water Distribution Network (WDN). The objective function includes terms for vertex costs,\n",
        "    edge interactions, and a penalty for deviating from the specified number of sensors.\n",
        "\n",
        "    Args:\n",
        "        G (nx.Graph): A NetworkX graph object representing the water distribution network (WDN) for the city.\n",
        "        model (pyo.ConcreteModel): A Pyomo ConcreteModel instance with sets and parameters defined.\n",
        "        s (int): The number of sensors to be placed in the network.\n",
        "        rho (float): Penalty parameter for the constraint that exactly `s` sensors must be placed.\n",
        "\n",
        "    Returns:\n",
        "        pyo.ConcreteModel: The updated Pyomo model with the QUBO objective function added.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the model is None.\n",
        "\n",
        "    Examples:\n",
        "        >>> model = QUBO(G, model, 5, 0.1)\n",
        "        >>> print(model.obj)\n",
        "    \"\"\"\n",
        "\n",
        "    if model_in is None:\n",
        "        return None\n",
        "\n",
        "    model = model_in.clone()\n",
        "\n",
        "    # # Removing previous objective function definition:\n",
        "    # if hasattr(model, 'obj'):\n",
        "    #     model.del_component(model.obj)\n",
        "    # model.obj = pyo.Objective(rule=objective_rule_MIQP, sense=pyo.minimize)\n",
        "\n",
        "    # # Removing previous constraint definition:\n",
        "    # if hasattr(model, 'sensor_constraint'):\n",
        "    #     model.del_component(model.sensor_constraint_MIQP)\n",
        "    # model.sensor_constraint_MIQP = pyo.Constraint(rule=sensor_constraint_rule_MIQP)\n",
        "\n",
        "\n",
        "    # Define objective function \n",
        "    def objective_rule_QUBO(model):\n",
        "        term1 = sum(model.c[i] * model.x[i] for i in model.nodes)\n",
        "        term2 = sum(model.w[(i, j)] * (1 - model.x[i]) * (1 - model.x[j]) for (i, j) in model.edges)\n",
        "        term3 = rho * (sum(model.x[i] for i in model.nodes) - s) ** 2\n",
        "        return term1 + term2 + term3\n",
        "\n",
        "    model.obj = pyo.Objective(rule=objective_rule_QUBO, sense=pyo.minimize)\n",
        "\n",
        "    return model\n",
        "\n",
        "    # No constraints for QUBO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coverage plot\n",
        "\n",
        "solver = pyo.SolverFactory('gurobi', solver_io='python')\n",
        "objective_value_list_MIP = []\n",
        "\n",
        "def coverage(G, s, rho):\n",
        "    \"\"\"Computes and plots coverage metrics for the Water Distribution Network (WDN) using both MIP and QUBO formulations.\n",
        "\n",
        "    This function iteratively solves the MIP and QUBO models for different numbers of sensors and records\n",
        "    the objective values. It uses the Gurobi solver to find the optimal solutions and appends the results\n",
        "    to lists for comparison.\n",
        "\n",
        "    Args:\n",
        "        G (nx.Graph): A NetworkX graph object representing the water distribution network (WDN) for the city.\n",
        "        s (int): The maximum number of sensors to be placed.\n",
        "        rho (float): Penalty parameter for the QUBO model.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two lists:\n",
        "            - objective_value_list_MIQP (list): A list of objective values obtained from the MIP formulation.\n",
        "            - objective_value_list_QUBO (list): A list of objective values obtained from the QUBO formulation.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the solver or model is not properly defined.\n",
        "\n",
        "    Examples:\n",
        "        >>> objective_value_list_MIQP, objective_value_list_QUBO = coverage(G, 5, 0.1)\n",
        "        >>> print(objective_value_list_MIQP)\n",
        "        [100, 95, 90]\n",
        "        >>> print(objective_value_list_QUBO)\n",
        "        [102, 96, 89]\n",
        "    \"\"\"\n",
        "\n",
        "    # MIP\n",
        "    for s in range(0, len(G.nodes())+1):\n",
        "        model_MIQP = MIQP(G, model, s)\n",
        "        # Solve MIP and append data to list for each iteration\n",
        "        results_MIQP = solver.solve(model_MIQP, tee=True) # MIP formulation result\n",
        "        objective_value_MIP = pyo.value(model_MIQP.obj)\n",
        "        print(\"Objective value from MIP formulation: \\n\", objective_value_MIP)\n",
        "        print(\"\\n\")\n",
        "        objective_value_list_MIP.append(objective_value_MIP)\n",
        "\n",
        "    return objective_value_list_MIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_coverage(s, objective_value_list_MIP):\n",
        "    \"\"\"\n",
        "    Plots the objective value against the number of sensors and marks the minimum objective value.\n",
        "\n",
        "    Args:\n",
        "        s (int): The maximum number of sensors to be placed.\n",
        "        objective_value_list_MIP (list): A list of objective values obtained from the MIP formulation.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Example:\n",
        "        >>> plot_coverage(5, [100, 95, 90, 85, 80])\n",
        "    \"\"\"\n",
        "    # Find the minimum objective value and its corresponding sensor count\n",
        "    min_obj_value = min(objective_value_list_MIP)\n",
        "    min_index = objective_value_list_MIP.index(min_obj_value)\n",
        "    \n",
        "    plt.figure(figsize=(16, 16))\n",
        "    plt.plot(range(len(objective_value_list_MIP)), objective_value_list_MIP, marker='o', linestyle='-', color='b', label='Objective Value')\n",
        "    \n",
        "    # Mark the minimum objective value\n",
        "    plt.plot(min_index, min_obj_value, marker='x', markersize=10, color='r', label=f'Min Objective Value: {min_obj_value}')\n",
        "    \n",
        "    plt.title('Objective Value vs Number of Sensors')\n",
        "    plt.xlabel('Number of Sensors (s)')\n",
        "    plt.ylabel('Objective Value')\n",
        "    plt.xticks(range(len(objective_value_list_MIP)))  # Ensures that the x-axis shows each integer sensor count\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "s = list(range(0, len(G.nodes())+1)) # List of feasible number of sensors that can be placed in the WDN\n",
        "rho = np.sum(np.abs(list(VC.values()))) + 1 # Constant for penalty term for QUBO\n",
        "objective_value_list_MIP = coverage(G, s, rho)\n",
        "plot_coverage(s, objective_value_list_MIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select optimal parameters from coverage plot results\n",
        "\n",
        "# Get the number of sensors as the minimum value in the objective_value_list_MIP list\n",
        "objective_value_list_MIP_min = min(objective_value_list_MIP)\n",
        "s = objective_value_list_MIP.index(objective_value_list_MIP_min)\n",
        "print(s)\n",
        "\n",
        "# s = 15 # Optimal number of sensors\n",
        "rho = np.sum(np.abs(list(VC.values()))) + 1 # Optimal constant for penalty term for QUBO\n",
        "\n",
        "model_MIQP = MIQP(G, model, s)\n",
        "model_QUBO = QUBO(G, model, s, rho)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_MIQP.pprint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solve\n",
        "solver = pyo.SolverFactory('gurobi', solver_io='python')\n",
        "results_MIQP = solver.solve(model_MIQP, tee=True) # MIP formulation result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_QUBO = solver.solve(model_QUBO, tee=True) # QUBO formulation result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sensor_placement_results(model_MIQP, model_QUBO):\n",
        "\n",
        "    \"\"\"Prints and returns sensor placement results for MIP and QUBO models.\n",
        "\n",
        "    This function displays the sensor placement status for each node in both MIP and QUBO models. It provides\n",
        "    a clear view of which nodes have sensors placed according to each optimization approach. It also returns\n",
        "    dictionaries with sensor placement information for further analysis.\n",
        "\n",
        "    Args:\n",
        "        model_MIQP (pyo.ConcreteModel): The Pyomo model with MIP formulation, including sensor placement variables.\n",
        "        model_QUBO (pyo.ConcreteModel): The Pyomo model with QUBO formulation, including sensor placement variables.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two dictionaries:\n",
        "            - sensor_placement_MIQP (dict): Dictionary with node names as keys and binary values (0 or 1) indicating\n",
        "              whether a sensor is placed at each node according to the MIP model.\n",
        "            - sensor_placement_QUBO (dict): Dictionary with node names as keys and binary values (0 or 1) indicating\n",
        "              whether a sensor is placed at each node according to the QUBO model.\n",
        "\n",
        "    Examples:\n",
        "        >>> sensor_placement_MIQP, sensor_placement_QUBO = sensor_placement_results(model_MIQP, model_QUBO)\n",
        "        >>> print(sensor_placement_MIQP)\n",
        "        {'J1': 1, 'J2': 0, 'J3': 1}\n",
        "        >>> print(sensor_placement_QUBO)\n",
        "        {'J1': 1, 'J2': 1, 'J3': 0}\n",
        "    \"\"\"\n",
        "\n",
        "    # Print results for MIP\n",
        "    print(\"MIP results\")\n",
        "    for node in model_MIQP.nodes:\n",
        "        print(f\"Node {node}: Sensor placed = {pyo.value(model_MIQP.x[node])}\")\n",
        "\n",
        "    sensor_placement_MIQP = {node: pyo.value(model_MIQP.x[node]) for node in model_MIQP.nodes}\n",
        "\n",
        "    # Print results for QUBO\n",
        "    print(\"\\nQUBO results\")\n",
        "    for node in model_QUBO.nodes:\n",
        "        print(f\"Node {node}: Sensor placed = {pyo.value(model_QUBO.x[node])}\")\n",
        "\n",
        "    sensor_placement_QUBO = {node: pyo.value(model_QUBO.x[node]) for node in model_QUBO.nodes}\n",
        "\n",
        "    return sensor_placement_MIQP, sensor_placement_QUBO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sensor_placement_MIQP, sensor_placement_QUBO = sensor_placement_results(model_MIQP, model_QUBO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_sensor_placement(city, sensor_placement, title): # subplot_index, \n",
        "\n",
        "    \"\"\"Plots a city's Water Distribution Network (WDN) and the optimal sensor placement.\n",
        "\n",
        "    This function visualizes the WDN graph with nodes colored based on sensor placement: red for nodes with\n",
        "    sensors and sky blue for nodes without. It uses Matplotlib to create the plot and adds a legend to distinguish\n",
        "    between sensor and non-sensor nodes.\n",
        "\n",
        "    Args:\n",
        "        city (str): The name of the city for which the WDN is plotted.\n",
        "        sensor_placement (dict): A dictionary indicating sensor placement at each node (1 for sensor, 0 for no sensor).\n",
        "        subplot_index (int): The index of the subplot where the WDN will be plotted (1 or 2).\n",
        "        title (str): The title of the plot.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the graph `G` is None.\n",
        "\n",
        "    Examples:\n",
        "        >>> plot_sensor_placement('Apulian', {'J1': 1, 'J2': 0, 'J3': 1}, 1, 'Optimal Sensor Placement')\n",
        "    \"\"\"\n",
        "\n",
        "    if G is None:\n",
        "        print(f\"Graph could not be constructed for {city}.\")\n",
        "        return\n",
        "\n",
        "    pos = nx.get_node_attributes(G, 'pos')\n",
        "\n",
        "    # Create plot\n",
        "    # plt.subplot(2, 1, subplot_index)\n",
        "    plt.plot(1, 1)\n",
        "    \n",
        "    # Determine node colors based on sensor placement\n",
        "    node_colors = ['red' if sensor_placement.get(node, 0) == 1 else 'skyblue' for node in G.nodes()]\n",
        "    \n",
        "    nx.draw_networkx_nodes(G, pos, node_size=500, node_color=node_colors)\n",
        "    nx.draw_networkx_edges(G, pos, edge_color='black')\n",
        "    #nx.draw_networkx_labels(G, pos, font_size=6, font_color='black')\n",
        "\n",
        "    # Round off edge and node labels to two decimals for the plot\n",
        "    edge_labels_rounded = {k: round(v, 2) for k, v in EB.items()}\n",
        "    node_labels_rounded = {k: round(v, 2) for k, v in VC.items()}\n",
        "\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels_rounded, label_pos=0.5, font_size=6, verticalalignment='bottom')\n",
        "    nx.draw_networkx_labels(G, pos, labels=node_labels_rounded, font_size=8, font_color='black')\n",
        "\n",
        "    # Add legend\n",
        "    red_patch = Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Sensor placed')\n",
        "    blue_patch = Line2D([0], [0], marker='o', color='w', markerfacecolor='skyblue', markersize=10, label='No sensor')\n",
        "    plt.legend(handles=[red_patch, blue_patch], loc='best', fontsize='24')\n",
        "\n",
        "    #plt.title(title, fontsize=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_comparison(city, sensor_placement_MIQP, save_path=None): # sensor_placement_QUBO,\n",
        "\n",
        "    \"\"\"Plots a city's Water Distribution Network (WDN) with optimal sensor placements from both MIP and QUBO formulations.\n",
        "\n",
        "    This function generates a single figure with two subplots, showing the WDN with sensor placements obtained\n",
        "    through the MIP and QUBO models. It utilizes the `plot_sensor_placement` function to visualize the results.\n",
        "\n",
        "    Args:\n",
        "        city (str): The name of the city for which the WDN is plotted.\n",
        "        sensor_placement_MIQP (dict): A dictionary indicating sensor placement at each node from the MIP formulation.\n",
        "        sensor_placement_QUBO (dict): A dictionary indicating sensor placement at each node from the QUBO formulation.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Examples:\n",
        "        >>> plot_comparison('Apulian', {'J1': 1, 'J2': 0, 'J3': 1}, {'J1': 0, 'J2': 1, 'J3': 1})\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the 'plots' directory if it doesn't exist\n",
        "    if save_path:\n",
        "        save_dir = os.path.dirname(save_path)\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "    # Save plot\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "    plt.figure(figsize=(24, 16))\n",
        "\n",
        "    plot_sensor_placement(city, sensor_placement_MIQP,  f'Optimal Sensor Placement - {city}')\n",
        "\n",
        "    # plot_sensor_placement(city, sensor_placement_MIQP, 1, f'Sensor Placement - {city} - MIP formulation')\n",
        "    # plot_sensor_placement(city, sensor_placement_QUBO, 2, f'Sensor Placement - {city} - QUBO formulation')\n",
        "\n",
        "    # Save plot\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_path = os.path.join('plots', 'sensor_placement_comparison.png')\n",
        "plot_comparison(city, sensor_placement_MIQP, save_path=save_path) # sensor_placement_QUBO, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdwVRu6_GD9X"
      },
      "source": [
        "## Simulated Annealing:\n",
        "First, we build the adjacency matrix $Q$ that is obtained from the QUBO model. Remember that the QUBO problem is formulated as follows: \n",
        "$$\n",
        "\\begin{array}{rl}\n",
        "    \\displaystyle%\n",
        "    \\min_{x} \\left( \\sum_{i \\in V} (c_i + \\rho - 2\\rho s - \\sum_{j \\in E} w_{ij})x_i^2 + \\sum_{(i,j) \\in E} w_{ij}x_i x_j + \\rho s^2 + \\sum_{(i,j) \\in E}{w_{ij}} \\right) \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "where $s$ is the total number of sensors that is predefined. And, We know that in general, a QUBO problem is defined as: \n",
        "$$\n",
        "\\min_{x \\in \\{0,1 \\}^n} \\sum_{(ij) \\in E} Q_{ij}x_i x_j + \\sum_{i \\in V}Q_{ii}x_i + c_Q = \\min_{x \\in \\{0,1 \\}^n}  x^\\top Q x + c_Q\n",
        "$$\n",
        "where we optimize over binary variables $x \\in \\{ 0,1 \\}^n$, on a constrained graph $G(V,E)$ defined by an adjacency matrix $Q$. We also include an arbitrary offset  $c_Q$.\n",
        "\n",
        "\n",
        "Comparing the above problems, it is evident that \n",
        "$$\n",
        "\\begin{array}{rl}\n",
        "    \\displaystyle%\n",
        "    Q_{ii} = (c_{i} + \\rho -2\\rho s) - \\sum_{j \\in V}w_{ij} \\\\\n",
        "    \n",
        "    Q_{ij} = w_{ij} \\\\ \\\\\n",
        "\n",
        "    c_{Q} = \\rho s^2 + \\sum_{(i,j) \\in E}{w_{ij}}\n",
        "\\end{array}\n",
        "$$\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_Q_matrix(G, s):\n",
        "\n",
        "    \"\"\"Builds the Q matrix for the Quadratic Unconstrained Binary Optimization (QUBO) problem based on the given graph and model.\n",
        "\n",
        "    This function constructs the Q adjacency matrix used in QUBO formulations, incorporating vertex costs, edge weights (edge betweenness), and constraints.\n",
        "\n",
        "    Args:\n",
        "        G (nx.Graph): The NetworkX graph object representing the water distribution network (WDN).\n",
        "        model (pyo.ConcreteModel): The Pyomo model with the necessary parameters for QUBO.\n",
        "        s (int): The number of sensors to be placed.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - Q (np.ndarray): The Q matrix for the QUBO problem.\n",
        "            - cQ (float): The constant term in the QUBO objective function.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the model is None.\n",
        "\n",
        "    Examples:\n",
        "        >>> Q, cQ = build_Q_matrix(G, model, 5)\n",
        "        >>> print(Q)\n",
        "        [[ 1.2 -0.5  0. ]\n",
        "         [-0.5  1.5 -0.3]\n",
        "         [ 0.  -0.3  1.1]]\n",
        "        >>> print(cQ)\n",
        "        2.5\n",
        "    \"\"\"\n",
        "\n",
        "    G = Construct_Graph(city)\n",
        "\n",
        "    num_nodes = len(G.nodes()) # stores the number of nodes in the graph G\n",
        "    nodes = list(G.nodes())\n",
        "\n",
        "    # Correctly create node_costs using node names\n",
        "    vertex_cost = np.array([VC[node] for node in nodes])\n",
        "    weight = EB\n",
        "\n",
        "    # Initialize the constraint matrix A and vector b\n",
        "    A = np.ones((1, num_nodes))\n",
        "    print(A)\n",
        "    b = np.array([s])\n",
        "    print(b)\n",
        "    \n",
        "    # Calculate rho\n",
        "    #rho = np.sum(np.abs(vertex_cost)) + 1\n",
        "    rho = 6\n",
        "    print(rho)\n",
        "  \n",
        "    # Add node costs to the diagonal\n",
        "    Q = np.diag(vertex_cost)\n",
        "    print(Q)\n",
        "    print(vertex_cost)\n",
        "    \n",
        "    # # Add edge weights to the Q matrix\n",
        "    total_weight = 0\n",
        "    print(weight)\n",
        "    for (i, j), w in weight.items():\n",
        "        if nodes.index(i) != nodes.index(j):\n",
        "            Q[nodes.index(i), nodes.index(j)] += w/2\n",
        "            Q[nodes.index(j), nodes.index(i)] += w/2\n",
        "        Q[nodes.index(i), nodes.index(i)] -= w\n",
        "        Q[nodes.index(j), nodes.index(j)] -= w\n",
        "        total_weight += w\n",
        "        print(i, j, w)\n",
        "    print(total_weight)\n",
        "    print(Q)\n",
        "    \n",
        "    # Adjust diagonal with constraints:\n",
        "    Q += rho*np.matmul(A.T,A)\n",
        "    print(Q)\n",
        "    Q -= rho*2*np.diag(np.matmul(b.T,A))\n",
        "    \n",
        "    # Calculate constant term cQ \n",
        "    cQ = rho * np.matmul(b.T, b) + total_weight\n",
        "    \n",
        "    return Q, cQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "s = 153\n",
        "Q, cQ = build_Q_matrix(G, s)\n",
        "print(\"Q matrix:\\n\", Q , \"\\n\")\n",
        "print(\"cQ value:\\n\", cQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "G = nx.from_numpy_matrix(Q)\n",
        "nx.draw(G, with_labels=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Binary Quadratic model (BQM)\n",
        "\n",
        "def QUBO_dimod(Q, beta):\n",
        "\n",
        "    \"\"\"Creates a Binary Quadratic Model (BQM) from a Q matrix and an offset.\n",
        "\n",
        "    This function constructs a BQM, which is used for solving quadratic optimization problems using binary variables.\n",
        "    It uses the `dimod` library to create the model from the Q adjacency matrix and an offset value, preparing it for use with\n",
        "    optimization solvers.\n",
        "\n",
        "    Args:\n",
        "        Q (np.ndarray): The Q matrix representing the quadratic terms in the optimization problem.\n",
        "        beta (float): The offset value for the BQM.\n",
        "\n",
        "    Returns:\n",
        "        dimod.BinaryQuadraticModel: The Binary Quadratic Model constructed from the Q matrix and offset.\n",
        "\n",
        "    Examples:\n",
        "        >>> Q = np.array([[1, -1], [-1, 2]])\n",
        "        >>> beta = 0.5\n",
        "        >>> bqm = QUBO_dimod(Q, beta)\n",
        "        >>> print(bqm)\n",
        "        BinaryQuadraticModel({0: 1, 1: -1}, {(0, 1): -1}, 0.5, dimod.BINARY)\n",
        "    \"\"\"\n",
        "\n",
        "    # Binary Quadratic Model\n",
        "    bqm = dimod.BinaryQuadraticModel.from_qubo(Q, offset = beta) \n",
        "    \n",
        "    return bqm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bqm = QUBO_dimod(Q, beta=cQ) # returns bqm value\n",
        "print(bqm)\n",
        "\n",
        "# # Use ExactSolver to sample the BQM\n",
        "# exactSampler = dimod.reference.samplers.ExactSolver()\n",
        "# exactSamples = exactSampler.sample(bqm)\n",
        "\n",
        "# # Print the sample results\n",
        "# print(\"Exact Solver Samples:\")\n",
        "# print(exactSamples)\n",
        "\n",
        "# # If you want to extract and process the samples:\n",
        "# for sample, energy in exactSamples.data(['sample', 'energy']):\n",
        "#     print(sample, \"Energy:\", energy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7AKOTXPGD9X"
      },
      "outputs": [],
      "source": [
        "# Some useful functions to get plots\n",
        "def plot_enumerate(results, title=None):\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    energies = [datum.energy for datum in results.data(\n",
        "        ['energy'], sorted_by='energy')]\n",
        "\n",
        "    if results.vartype == 'Vartype.BINARY':\n",
        "        samples = [''.join(c for c in str(datum.sample.values()).strip(\n",
        "            ', ') if c.isdigit()) for datum in results.data(['sample'], sorted_by=None)]\n",
        "        plt.xlabel('bitstring for solution')\n",
        "    else:\n",
        "        samples = np.arange(len(energies))\n",
        "        plt.xlabel('solution')\n",
        "\n",
        "    plt.bar(samples,energies)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.ylabel('Energy')\n",
        "    plt.title(str(title))\n",
        "    print(\"minimum energy:\", min(energies))\n",
        "\n",
        "def plot_samples(results, title=None):\n",
        "    plt.figure()\n",
        "    if results.vartype == 'Vartype.BINARY':\n",
        "        samples = [''.join(c for c in str(datum.sample.values()).strip(\n",
        "            ', ') if c.isdigit()) for datum in results.data(['sample'], sorted_by=None)]\n",
        "        plt.xlabel('bitstring for solution')\n",
        "    else:\n",
        "        samples = np.arange(len(energies))\n",
        "        plt.xlabel('solution')\n",
        "\n",
        "    counts = Counter(samples)\n",
        "    total = len(samples)\n",
        "    for key in counts:\n",
        "        counts[key] /= total\n",
        "    df = pd.DataFrame.from_dict(counts, orient='index').sort_index()\n",
        "    df.plot(kind='bar', legend=None)\n",
        "\n",
        "    plt.xticks(rotation=80)\n",
        "    plt.ylabel('Probabilities')\n",
        "    plt.title(str(title))\n",
        "    plt.show()\n",
        "    print(\"minimum energy:\", min(energies))\n",
        "\n",
        "\n",
        "def plot_energies(results, title=None, skip=1):\n",
        "    # skip parameter given to avoid putting all xlabels\n",
        "    energies = results.data_vectors['energy']\n",
        "    occurrences = results.data_vectors['num_occurrences']\n",
        "    counts = Counter(energies)\n",
        "    total = sum(occurrences)\n",
        "    counts = {}\n",
        "    for index, energy in enumerate(energies):\n",
        "        if energy in counts.keys():\n",
        "            counts[energy] += occurrences[index]\n",
        "        else:\n",
        "            counts[energy] = occurrences[index]\n",
        "    for key in counts:\n",
        "        counts[key] /= total\n",
        "    df = pd.DataFrame.from_dict(counts, orient='index').sort_index()\n",
        "    ax = df.plot(kind='bar', legend=None)\n",
        "\n",
        "    plt.xlabel('Energy')\n",
        "    plt.ylabel('Probabilities')\n",
        "    # Plot only a subset of xlabels (every skip steps)\n",
        "    ax.set_xticklabels([t if not i%skip else \"\" for i,t in enumerate(ax.get_xticklabels())])\n",
        "    plt.title(str(title))\n",
        "    plt.show()\n",
        "    print(\"minimum energy:\", min(energies))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulated Annealing \n",
        "mod = bqm\n",
        "simAnnSampler = neal.SimulatedAnnealingSampler()\n",
        "start_SA = time.time()\n",
        "simAnnSamples = simAnnSampler.sample(mod, num_reads=4000)\n",
        "end_SA = time.time()\n",
        "total_time_SA = end_SA - start_SA\n",
        "print(\"Total time for Simulated Annealing:\", total_time_SA)\n",
        "plot_enumerate(simAnnSamples, title='Simulated annealing in default parameters')\n",
        "plot_energies(simAnnSamples, title='Simulated annealing in default parameters')\n",
        "simAnnSamples.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tabu search\n",
        "start_tabu = time.time()\n",
        "sampleset_tabu = TabuSampler().sample(mod, num_reads=4000)\n",
        "end_tabu = time.time()\n",
        "total_time_tabu = end_tabu - start_tabu\n",
        "print(\"Total time for Tabu search:\", total_time_tabu)\n",
        "plot_enumerate(sampleset_tabu, title='Tabu search in default parameters')\n",
        "plot_energies(sampleset_tabu, title='Tabu search in default parameters')\n",
        "sampleset_tabu.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDoGrypYGD9Y"
      },
      "outputs": [],
      "source": [
        "# plot_enumerate(exactSamples, title='Enumerate all solutions')\n",
        "# plot_energies(exactSamples, title='Enumerate all solutions', skip=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(simAnnSamples)\n",
        "print(simAnnSamples.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(sampleset_tabu)\n",
        "print(sampleset_tabu.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quantum Annealing via D-Wave:\n",
        "This is the first interaction with D-Wave's Quantum Annealer. It will use the QUBO model introduced earlier and will define it using D-Wave's package dimod, and then solve them using neal's implementation of simulated annealing classicaly and D-Wave system package to use Quantum Annealing. We will also leverage the use of Networkx for network models/graphs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import DWave\n",
        "import dwave_networkx as dnx\n",
        "from dwave.system import (DWaveSampler, EmbeddingComposite,\n",
        "                          FixedEmbeddingComposite)\n",
        "from pprint import pprint\n",
        "from dwave.system import LeapHybridSampler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph corresponding to D-Wave Model\n",
        "qpu = DWaveSampler()\n",
        "qpu_edges = qpu.edgelist\n",
        "qpu_nodes = qpu.nodelist\n",
        "# pprint(dir(qpu))\n",
        "if qpu.solver.id == \"DW_2000Q_6\":\n",
        "    print(qpu.solver.id)\n",
        "    X = dnx.chimera_graph(16, node_list=qpu_nodes, edge_list=qpu_edges)\n",
        "    dnx.draw_chimera(X, node_size=1)\n",
        "    print('Number of qubits=', len(qpu_nodes))\n",
        "    print('Number of couplers=', len(qpu_edges))\n",
        "else:\n",
        "    print(qpu.solver.id)\n",
        "    X = dnx.pegasus_graph(16, node_list=qpu_nodes, edge_list=qpu_edges)\n",
        "    dnx.draw_pegasus(X, node_size=1)\n",
        "    print('Number of qubits=', len(qpu_nodes))\n",
        "    print('Number of couplers=', len(qpu_edges))\n",
        "     \n",
        "DWavesampler = EmbeddingComposite(DWaveSampler())\n",
        "DWaveSamples = DWavesampler.sample(bqm, num_reads=3500, \n",
        "                                   return_embedding=True, \n",
        "                                  #  chain_strength=chain_strength, \n",
        "                                  #  annealing_time=annealing_time\n",
        ")\n",
        "print(DWaveSamples.info)\n",
        "embedding = DWaveSamples.info['embedding_context']['embedding']\n",
        "if qpu.solver.id == \"DW_2000Q_6\":\n",
        "  dnx.draw_chimera_embedding(X, embedding, node_size=2)\n",
        "else:\n",
        "  dnx.draw_pegasus_embedding(X, embedding, node_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_enumerate(DWaveSamples, title='Quantum annealing in default parameters')\n",
        "plot_energies(DWaveSamples, title='Quantum annealing in default parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(DWaveSamples)\n",
        "print(DWaveSamples.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leap Hybrid Solver\n",
        "sampler = LeapHybridSampler(solver={'category': 'hybrid'})\n",
        "sampleset_Leap = sampler.sample(bqm)\n",
        "sampleset_Leap.info  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_enumerate(sampleset_Leap, title='hybrid quantum-classical in default parameters')\n",
        "plot_energies(sampleset_Leap, title='hybrid quantum-classical in default parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_samples(simAnnSamples, DWaveSamples, sampleset_tabu, sampleset_Leap):\n",
        "    \"\"\"\n",
        "    Processes D-Wave's Simulated and Quantum Annealing samplesets to calculate frequency, \n",
        "    mean energy, and probability for each energy level.\n",
        "\n",
        "    This function converts D-Wave's Simulated and Quantum Annealing samplesets to pandas \n",
        "    DataFrames, aggregates the results by grouping by sample values, calculates the \n",
        "    frequency and mean energy for each group, and then calculates the probability for \n",
        "    each energy level. The resulting DataFrames are sorted by mean energy in ascending \n",
        "    order.\n",
        "\n",
        "    Args:\n",
        "        simAnnSamples: A sampleset object from which the D-Wave Simulated Annealing data will \n",
        "                       be converted into a pandas DataFrame. The sampleset is expected \n",
        "                       to contain 'energy' and 'num_occurrences' columns.\n",
        "        DWaveSamples: A sampleset object from which the D-Wave Quantum Annealing data will be converted \n",
        "                      into a pandas DataFrame. The sampleset is expected to contain \n",
        "                      'energy' and 'num_occurrences' columns.\n",
        "        TabuSamples: A sampleset object from which the Tabu search data will be converted\n",
        "                        into a pandas DataFrame. The sampleset is expected to contain\n",
        "                        'energy' and 'num_occurrences' columns.\n",
        "        LeapSamples: A sampleset object from which the Leap Hybrid Solver data will be converted\n",
        "                        into a pandas DataFrame. The sampleset is expected to contain\n",
        "                        'energy' and 'num_occurrences' columns.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two pandas DataFrames:\n",
        "            - simAnn_df: DataFrame with mean energy and probability for Simulated Annealing.\n",
        "            - dwave_df: DataFrame with mean energy and probability for D-Wave.\n",
        "            - tabu_df: DataFrame with mean energy and probability for Tabu search.\n",
        "            - leap_df: DataFrame with mean energy and probability for Leap Hybrid Solver.\n",
        "\n",
        "    Example:\n",
        "        simAnn_df, dwave_df,  = process_samples(simAnnSamples, DWaveSamples)\n",
        "    \"\"\"\n",
        "    # Convert Simulated Annealing sampleset to pandas dataframe\n",
        "    SimAnn_Samples_df = simAnnSamples.to_pandas_dataframe()\n",
        "\n",
        "    # Aggregate the results for Simulated Annealing\n",
        "    grouped_simAnn = SimAnn_Samples_df.groupby(SimAnn_Samples_df.columns.difference(['energy', 'num_occurrences']).tolist())\n",
        "    result_simAnn = grouped_simAnn.agg(\n",
        "        frequency=('num_occurrences', 'sum'),\n",
        "        mean_energy=('energy', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Calculate the total number of Simulated Annealing samples\n",
        "    total_samples_simAnn = result_simAnn['frequency'].sum()\n",
        "\n",
        "    # Calculate the probability for each energy level in Simulated Annealing\n",
        "    result_simAnn['probability'] = result_simAnn['frequency'] / total_samples_simAnn\n",
        "\n",
        "    # Sort the Simulated Annealing results by mean_energy in ascending order\n",
        "    simAnn_df = result_simAnn.sort_values(by='mean_energy')\n",
        "\n",
        "    # Convert D-Wave sampleset to pandas dataframe\n",
        "    DWave_Samples_df = DWaveSamples.to_pandas_dataframe()\n",
        "\n",
        "    # Aggregate the results for D-Wave\n",
        "    grouped_DWave = DWave_Samples_df.groupby(DWave_Samples_df.columns.difference(['energy', 'num_occurrences']).tolist())\n",
        "    result_DWave = grouped_DWave.agg(\n",
        "        frequency=('num_occurrences', 'sum'),\n",
        "        mean_energy=('energy', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Calculate the total number of D-Wave samples\n",
        "    total_samples_DWave = result_DWave['frequency'].sum()\n",
        "\n",
        "    # Calculate the probability for each energy level in D-Wave\n",
        "    result_DWave['probability'] = result_DWave['frequency'] / total_samples_DWave\n",
        "\n",
        "    # Sort the D-Wave results by mean_energy in ascending order\n",
        "    dwave_df = result_DWave.sort_values(by='mean_energy')\n",
        "    \n",
        "    # Convert tabu search sampleset to pandas dataframe\n",
        "    tabu_Samples_df = sampleset_tabu.to_pandas_dataframe()\n",
        "\n",
        "    # Aggregate the results for tabu search\n",
        "    grouped_tabu = tabu_Samples_df.groupby(tabu_Samples_df.columns.difference(['energy', 'num_occurrences']).tolist())\n",
        "    result_tabu = grouped_tabu.agg(\n",
        "        frequency=('num_occurrences', 'sum'),\n",
        "        mean_energy=('energy', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Calculate the total number of tabu search samples\n",
        "    total_samples_tabu = result_tabu['frequency'].sum()\n",
        "\n",
        "    # Calculate the probability for each energy level in tabu search\n",
        "    result_tabu['probability'] = result_tabu['frequency'] / total_samples_tabu\n",
        "\n",
        "    # Sort the tabu search results by mean_energy in ascending order\n",
        "    tabu_df = result_tabu.sort_values(by='mean_energy')\n",
        "\n",
        "    # Convert Leap Hybrid Solver sampleset to pandas dataframe\n",
        "    leap_Samples_df = sampleset_Leap.to_pandas_dataframe()\n",
        "\n",
        "    # Aggregate the results for Leap Hybrid Solver\n",
        "    grouped_leap = leap_Samples_df.groupby(leap_Samples_df.columns.difference(['energy', 'num_occurrences']).tolist())\n",
        "    result_leap = grouped_leap.agg(\n",
        "        frequency=('num_occurrences', 'sum'),\n",
        "        mean_energy=('energy', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Calculate the total number of Leap Hybrid Solver samples\n",
        "    total_samples_leap = result_leap['frequency'].sum()\n",
        "\n",
        "    # Calculate the probability for each energy level in Leap Hybrid Solver\n",
        "    result_leap['probability'] = result_leap['frequency'] / total_samples_leap\n",
        "\n",
        "    # Sort the Leap Hybrid Solver results by mean_energy in ascending order\n",
        "    leap_df = result_leap.sort_values(by='mean_energy')\n",
        "\n",
        "    # Print the dataframes\n",
        "    pd.set_option('display.max_rows', None)  # Ensure all rows are printed\n",
        "    print(\"Tabu Search Results:\")\n",
        "    print(tabu_df[['mean_energy', 'probability']])\n",
        "    print(\"\\nLeap Hybrid Solver Results:\")\n",
        "    print(leap_df[['mean_energy', 'probability']])\n",
        "\n",
        "    return simAnn_df, dwave_df, tabu_df, leap_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = process_samples(simAnnSamples, DWaveSamples, sampleset_tabu, sampleset_Leap)\n",
        "simAnn_df, dwave_df, tabu_df, leap_df = results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_energy_probability(dwave_df, simAnn_df, tabu_df, leap_df, num_samples=10, width=0.3, line_x=4.871023058796, y_max=0.002):\n",
        "    df_list = [dwave_df, simAnn_df, tabu_df, leap_df]  # Updated list of dataframes\n",
        "    df_names = ['Quantum Annealing', 'Simulated Annealing', 'Tabu Search', 'Leap Hybrid Solver']  # Updated names\n",
        "    df_colors = ['blue', 'red', 'yellow', 'purple']  # Updated colors\n",
        "    \n",
        "    # Filter each dataframe to include only the first `num_samples` samples\n",
        "    filtered_dwave_df = dwave_df.head(num_samples)\n",
        "    filtered_simAnn_df = simAnn_df.head(num_samples)\n",
        "    filtered_tabu_df = tabu_df.head(num_samples)\n",
        "    filtered_leap_df = leap_df.head(num_samples)\n",
        "\n",
        "    # Round the mean_energy values to 4 decimal places\n",
        "    filtered_dwave_df['mean_energy'] = filtered_dwave_df['mean_energy'].round(4)\n",
        "    filtered_simAnn_df['mean_energy'] = filtered_simAnn_df['mean_energy'].round(4)\n",
        "    filtered_tabu_df['mean_energy'] = filtered_tabu_df['mean_energy'].round(4)\n",
        "    filtered_leap_df['mean_energy'] = filtered_leap_df['mean_energy'].round(4)\n",
        "\n",
        "    # Define the x-axis range from 2 to 3.2\n",
        "    x_range = np.arange(3, 10)\n",
        "\n",
        "    # Combine x_range with specific values from the data\n",
        "    x_labels = sorted(set(filtered_dwave_df['mean_energy']).union(set(filtered_simAnn_df['mean_energy'])).union(set(filtered_tabu_df['mean_energy'])).union(set(filtered_leap_df['mean_energy'])).union(set(x_range)))\n",
        "\n",
        "    # Calculate positions for bars for both datasets\n",
        "    pos_dwave = [x_labels.index(val) for val in filtered_dwave_df['mean_energy']]\n",
        "    pos_simAnn = [x_labels.index(val) for val in filtered_simAnn_df['mean_energy']]\n",
        "    pos_tabu = [x_labels.index(val) for val in filtered_tabu_df['mean_energy']]\n",
        "    pos_leap = [x_labels.index(val) for val in filtered_leap_df['mean_energy']]\n",
        "\n",
        "    # Create a bar plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    # Create bars for dwave_df data\n",
        "    plt.bar(np.array(pos_dwave) - width / 2, \n",
        "            filtered_dwave_df['probability'], \n",
        "            width, \n",
        "            color=df_colors[0], \n",
        "            label=df_names[0], \n",
        "            align='center', \n",
        "            alpha=0.7)  # Adjust transparency for better visibility\n",
        "\n",
        "    # Create bars for simAnn_df data\n",
        "    plt.bar(np.array(pos_simAnn) + width / 2, \n",
        "            filtered_simAnn_df['probability'],\n",
        "            width, \n",
        "            color=df_colors[1], \n",
        "            label=df_names[1], \n",
        "            align='center', \n",
        "            alpha=0.7)  # Adjust transparency for better visibility\n",
        "\n",
        "    # Create bars for tabu_df data\n",
        "    plt.bar(np.array(pos_tabu) + width / 2,\n",
        "            filtered_tabu_df['probability'],\n",
        "            width,\n",
        "            color=df_colors[2],\n",
        "            label=df_names[2],\n",
        "            align='center',\n",
        "            alpha=0.7)  # Adjust transparency for better visibility\n",
        "\n",
        "    # Create bars for leap_df data\n",
        "    plt.bar(np.array(pos_leap) + width / 2,\n",
        "            filtered_leap_df['probability'],\n",
        "            width,\n",
        "            color=df_colors[3],\n",
        "            label=df_names[3],\n",
        "            align='center',\n",
        "            alpha=0.7)  # Adjust transparency for better visibility\n",
        "            \n",
        "    # Add a dashed vertical line at objective vaue obtained through MIP formulation\n",
        "    if line_x in x_labels:\n",
        "        ax.axvline(x=x_labels.index(line_x), color='green', linestyle='--', label='Gurobi')\n",
        "    else:\n",
        "        # If the exact value is not in x_labels, use the closest index\n",
        "        nearest_idx = min(range(len(x_labels)), key=lambda i: abs(x_labels[i] - line_x))\n",
        "        ax.axvline(x=nearest_idx, color='green', linestyle='--', label='Gurobi')\n",
        "\n",
        "    # Set the y-axis label\n",
        "    ax.set_ylabel('Probability')\n",
        "    ax.set_xlabel('Mean Energy')\n",
        "\n",
        "    # Set the chart's title\n",
        "    ax.set_title(f'Energy vs Probability for Different Solvers - {city} WDN')\n",
        "\n",
        "    # Set the x-axis limits to ensure the vertical line is visible\n",
        "    ax.set_xlim(-0.5, len(x_labels) - 0.5)\n",
        "\n",
        "    # Set the y-axis limits to accommodate the desired range\n",
        "    ax.set_ylim(0, y_max)  # Adjust the upper limit as needed\n",
        "\n",
        "    # Set the x-axis ticks and labels\n",
        "    ax.set_xticks(np.arange(len(x_labels)))\n",
        "    ax.set_xticklabels([f\"{x_labels[i]:.4f}\" for i in np.arange(len(x_labels))], rotation=45, ha='right')\n",
        "\n",
        "    # Display the legend\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_energy_probability(dwave_df, simAnn_df, tabu_df, leap_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time To Solution (TTS):\n",
        "We would like to obtain a large probability of finding a right solution (the definition of right comes from what you define as success). On the other hand, the time it takes to solve these cases should be as small as possible. Therefore we are interested in a metric that combines both, and is called the Time To Solution (TTS) which is defined as\n",
        "$$\n",
        "TTS = T_{single run} * \\frac{\\log({1-s})}{\\log({1-p})}\n",
        "$$\n",
        "where s is a success factor, and taken as $s = 99\\%$, while $p$ is the success probability, usually accounted as the observed success probability.\n",
        "\n",
        "One usually reads this as the time to solution within 99\\% probability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def calculate_tts(samples, s, min_energy, TT):\n",
        "#     \"\"\"\n",
        "#     Calculates the Time-to-Solution (TTS) for a given set of samples.\n",
        "\n",
        "#     Args:\n",
        "#         samples (iterable): A collection of samples, each containing a 'sample', \n",
        "#                             'energy', and 'num_occurrences'.\n",
        "#         s (int): The expected total size for feasible solutions.\n",
        "#         energy_threshold (float): The energy threshold below which a solution is \n",
        "#                                   considered optimal.\n",
        "#         total_time (float): The total time used for calculations, typically representing \n",
        "#                             the time taken to run the optimization process.\n",
        "\n",
        "#     Returns:\n",
        "#         tuple: A tuple containing the following:\n",
        "#             - p_opt (float): The probability of finding an optimal solution.\n",
        "#             - p_fea (float): The probability of finding a feasible solution.\n",
        "#             - TTS_opt (float): The Time-to-Solution for finding an optimal solution.\n",
        "#             - TTS_fea (float): The Time-to-Solution for finding a feasible solution.\n",
        "\n",
        "#     Example:\n",
        "#         tts_results = calculate_tts(simAnnSamples.data(['sample', 'energy', 'num_occurrences']), 10, 2.68, total_time)\n",
        "#     \"\"\"\n",
        "#     p_opt = 0\n",
        "#     p_fea = 0\n",
        "    \n",
        "#     # Convert the generator to a list if needed\n",
        "#     samples_data = list(samples)\n",
        "#     total_samples = len(samples_data)\n",
        "    \n",
        "#     # Calculate total occurrences for feasible and optimal samples\n",
        "#     for sample, energy, num_ocu in samples_data:\n",
        "#         total_size = sum(sample.values())\n",
        "#         if total_size == s:\n",
        "#             print(total_size)\n",
        "#             p_fea += num_ocu\n",
        "#             if energy <= min_energy + 1e-6:\n",
        "#                 p_opt += num_ocu\n",
        "\n",
        "#     # Normalize by the total number of samples\n",
        "#     p_opt /= total_samples\n",
        "#     p_fea /= total_samples\n",
        "\n",
        "#     # Calculate TTS values\n",
        "#     TTS_opt = TT * np.log(1 - 0.99) / np.log(1 - p_opt)\n",
        "#     TTS_fea = TT * np.log(1 - 0.99) / np.log(1 - p_fea)\n",
        "\n",
        "#     return p_opt, p_fea, TTS_opt, TTS_fea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_tts(sample_set, s, energy_threshold, exec_time): \n",
        "    \"\"\"\n",
        "    Calculate the time to solution (optimality or feasibility) for the simulated annealing solver.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dist_sim : dimod.SampleSet\n",
        "        The samples returned by the simulated or quantum annealing solver.    \n",
        "    exec_time : float\n",
        "        The execution time of the simulated or quantum annealing solver.\n",
        "    energy_threshold : float\n",
        "        The energy threshold for the optimal or feasible solution.\n",
        "    s : float, optional\n",
        "        The success probability, by default 0.99\n",
        "    \"\"\"\n",
        "\n",
        "    # dist = sample_set.aggregate()\n",
        "    samples_data = list(sample_set)\n",
        "    tts = exec_time\n",
        "    total_size = s\n",
        "\n",
        "    # Define probability of finding solution in dist with energy <= energy_threshold \n",
        "    p_opt = 0\n",
        "    p_fea = 0\n",
        "    n = 0 # total number of samples\n",
        "\n",
        "    for sample, energy, num_ocu in samples_data:\n",
        "        #print(sample, energy, num_ocu)\n",
        "        total_size = sum(sample.values())\n",
        "        print(total_size)\n",
        "        if total_size == s:\n",
        "            p_fea += num_ocu\n",
        "        if energy <= energy_threshold:\n",
        "            p_opt += num_ocu\n",
        "\n",
        "        n += num_ocu\n",
        "        \n",
        "    # Normalize the probability\n",
        "    p_opt = p_opt/n\n",
        "    p_fea = p_fea/n\n",
        "    print(f'Probability of optimal solution <= {energy_threshold} : {p_opt}')\n",
        "    print(f'Probability of feasible solution sataisfying constraints : {p_fea}')\n",
        "\n",
        "    # Compute time to solution for optimal solution\n",
        "    if p_opt == 1:\n",
        "        TTS_opt = tts\n",
        "    elif p_opt == 0:\n",
        "        TTS_opt = np.inf\n",
        "    else:\n",
        "        TTS_opt = tts*np.log(1 - 0.99)/np.log(1 - p_opt)\n",
        "\n",
        "    # Compute time to solution for feasible solution\n",
        "    if p_fea == 1:\n",
        "        TTS_fea = tts\n",
        "    elif p_fea == 0:\n",
        "        TTS_fea = np.inf\n",
        "    else:\n",
        "        TTS_fea = tts*np.log(1 - 0.99)/np.log(1 - p_fea)\n",
        "\n",
        "    print(f'Time to solution for optimal solution : {TTS_opt}')\n",
        "    print(f'Time to solution for feasible solution : {TTS_fea}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TTS for Simulated Annealing samples\n",
        "tts_results_SA = calculate_tts(simAnnSamples.data(['sample', 'energy', 'num_occurrences']), 153, 28.25, total_time_SA)\n",
        "print(tts_results_SA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TTS for Tabu Search samples\n",
        "tts_results_Tabu = calculate_tts(sampleset_tabu.data(['sample', 'energy', 'num_occurrences']), 153, 12.154, total_time_tabu)\n",
        "print(tts_results_Tabu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TTS for Leap Hybrid Solver samples\n",
        "total_time_Leap = sampleset_Leap.info['run_time']\n",
        "print(total_time_Leap)\n",
        "tts_results_Leap = calculate_tts(sampleset_Leap.data(['sample', 'energy', 'num_occurrences']), 153, 22.841, total_time_Leap)\n",
        "print(tts_results_Leap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TTS for Quantum Annealing samples\n",
        "# Assign the correct key for the total time to total_time_QA\n",
        "total_time_QA = DWaveSamples.info['timing']['qpu_access_time']\n",
        "print(total_time_QA)\n",
        "tts_results_QA = calculate_tts(DWaveSamples.data(['sample', 'energy', 'num_occurrences']), 15, 610, total_time_QA)\n",
        "print(tts_results_QA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Provided data\n",
        "nodes = [24, 37, 114, 272, 856]\n",
        "gurobi_tts_MIP = [0.07, 0.06, 0.08, 0.03, 0.11]\n",
        "sim_ann_tts = [81.60481582, 96.50665403, 440010.0127, 2192982.025, 23942409.5]\n",
        "tabu_search_tts = [84.32313108, 84.47434545, 10733.85075, 5750.226611, 253908.6454]\n",
        "leap_hybrid_tts = [2.98621, 2.980085, 2.998373, 3.000249, 2.997319]\n",
        "quant_ann_tts = [8193.967296, 2869.983265, 13853.31548, np.inf, np.inf]  # Using np.inf for 'Infinity'\n",
        "\n",
        "# Define a high value to plot infinities\n",
        "high_value = max(max(gurobi_tts_MIP), max(sim_ann_tts), max(tabu_search_tts), max(leap_hybrid_tts)) * 10\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(24, 10))\n",
        "plt.plot(nodes, gurobi_tts_MIP, 'o-', label='Gurobi', color='green')\n",
        "plt.plot(nodes, sim_ann_tts, 's-', label='Simulated Annealing', color='red')\n",
        "plt.plot(nodes, tabu_search_tts, 'd-', label='Tabu Search', color='yellow')\n",
        "plt.plot(nodes, leap_hybrid_tts, 'x-', label='Leap Hybrid', color='maroon')\n",
        "\n",
        "# Plot Quantum Annealing with high value for infinity\n",
        "plt.plot(nodes, [high_value if x == np.inf else x for x in quant_ann_tts], 'v-', label='Quantum Annealing', color='blue')\n",
        "\n",
        "# Add annotations for infinity values\n",
        "for i, value in enumerate(quant_ann_tts):\n",
        "    if value == np.inf:\n",
        "        plt.text(nodes[i], high_value, 'âˆž', color='blue', ha='center', va='bottom')\n",
        "\n",
        "# Add dotted vertical lines and labels for each node\n",
        "for node in nodes:\n",
        "    plt.axvline(x=node, color='gray', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Customizing the x-axis ticks and labels\n",
        "plt.xlabel('Node Size of WDN', fontsize='20')\n",
        "plt.ylabel('TTS for Best Solution (s)', fontsize='20')\n",
        "# plt.xticks(fontsize=16)\n",
        "plt.xticks(nodes, fontsize=16)  # Set x-axis ticks to the exact node sizes with font size 12\n",
        "plt.yticks(fontsize=16)  # Set y-axis ticks font size to 12\n",
        "plt.legend(fontsize=16)  # Increase the font size of the legend\n",
        "plt.yscale('log')  # Using logarithmic scale due to large range of TTS values\n",
        "plt.ylim(0, high_value * 1.1)  # Extend the y-axis limit to accommodate the high values\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
